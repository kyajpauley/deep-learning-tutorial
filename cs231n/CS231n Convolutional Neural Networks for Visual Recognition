http://cs231n.github.io/

This is probably just going to be a notes file, will make directories as needed for organization

Jupyter notebooks
-trying to run notebooks in a directory other than the default directory is really annoying
-was able to get it to run in repo notebooks directory with following command
    jupyter notebook --notebook-dir=..\..\..\git-repos\deep-learning-tutorial\notebooks
    -I don't know if that saved the default dir to there, will see next time start up
-to run jupyter notebooks from winpython, go to winpython folder, click shortcut
    -can also cd to file and run jupyter notebook from there

-having problems getting kernels to connect while using winpython version
    -location of notebooks dir doesn't seem to matter
-retrying jupyter installation using anaconda
    -was able to get kernel working
    -have no idea how to get the notebook to start in preferred directory...

was finally able to fix target dir to start notebooks
    -open cwp.py file in anaconda directory
    -near end of file there is a line like this: os.chdir(documents_folder)
    -commented it out, changed to os.chdir('E:\\git-repos\\deep-learning-tutorial\\notebooks')
    -yay it works now



Image Classification
-computer sees images as matrix of pixels with RGB values
    -pixels_wide * pixels_tall * [R, G, B]
        -RGB are color channel values, integer from 0 (black) to 255 (white)
    -so basically, at pixel xy, the color is rbg
        -3d array of brightness values  --->    image label
-there are a lot of possible variations in how we perceive visual content
    -viewpoint - orientation with respect to camera
    -scale - size
    -deformation - things can be deformed/look different than usual
    -occlusion - parts may be hidden
    -illumination - lights affect pixel brightness
    -background clutter - object can blend into environment, harder to identify
    -intra-class variation - cats look different but still cats
-training dataset of labeled images
    -n = num images, k = num classes

-using pixel differences to compare images sucks because ultimately will group things based on similar colors
    -matrix is a list of pixel colors, so color is the biggest deciding factor

-nearest neighbor classifier
    -take test image, compare to all of training images, return label of closest training image
        -L1 distance - the sum of the differences between all vectors in two matrices; absolute value
        d1(I1,I2)=∑p|Ip1−Ip2|
            -pretty crappy classifier, better than at random but not human quality
        -L2 distance - euclidian distance between vectors, compute pixelwise differences, square them, then get sum, then get square root of sum
        d2(I1,I2)= sqrt(∑p((Ip1−Ip2)**2))
            -doesn't perform much better in image classification
            -L2 metric is more harsh than L1
    -has no training time, since just compare test objects to training objects
        -computationally expensive at test time
            -want most of load at training and efficient test instead

-k-nearest neighbor classifier
    -find top k closest images
        -return most frequent label? not sure if it's a pure frequency count
    -if k = 1, is basically just nearest neighbor
    -can use for hyperparameter tweaking
        -determine what value of k (num nearest neighbors to compare for prediction) gets best accuracy

-k-fold cross validation
    -if size of dataset is very small
    -divide dataset into k pieces, choose 1 for validation, use rest for training
        -iterate over all folds as validation, get eval, average performance across folds
        -only works if dataset is small because otherwise time consuming and computationally expensive
            if size of validation set small enough to quickly train/test on
                -for images probably 100-500 images
                -small text lines - about 1m lines

Bias term
-bias term is what prevents f(xi) = 0 for resulting algorithm, weight matrix in shape k * 1 (k = num classes)
    -bias vector influences output scores for each class, but does not interact with data
        f(xi,W,b)=Wxi+b = linear mapping
        -basically prevents the classifier from being unable to make a decision on which class to pick, because each class has a bonus (can it have negative bonus?) probability of being the class
    -can set a default bias of 1 so don't have to keep track of bias weights
        -function becomes f(xi,W)=Wxi

Loss function
-loss function wants to minimize the differences between model's predictions and ground truth labels
    -adjusts hyperparameters until highest accuracy parameters found
-SVM loss
    -goal = correct class for image should have higher score than incorrect class by fixed margin delta
    Li=∑(j≠yi)max(0, sj − syi + Δ)
        -compute the loss for the i-th example using score for the j-th class
                -ultimately minimize loss by picking the class with the least shitty score
            -vector of class scores
            -syi = score of yi, true (gold) class score
            -sj = score of j-th class instance, predicted class score
            -Δ = its the fucking delta mate
        takes max between 0 or sum of difference between correct class score and predicted class score + delta
            -if correct class, should have , should have significantly higher score