http://cs231n.github.io/

This is probably just going to be a notes file, will make directories as needed for organization

Jupyter notebooks
-trying to run notebooks in a directory other than the default directory is really annoying
-was able to get it to run in repo notebooks directory with following command
    jupyter notebook --notebook-dir=..\..\..\git-repos\deep-learning-tutorial\notebooks
    -I don't know if that saved the default dir to there, will see next time start up
-to run jupyter notebooks from winpython, go to winpython folder, click shortcut
    -can also cd to file and run jupyter notebook from there

-having problems getting kernels to connect while using winpython version
    -location of notebooks dir doesn't seem to matter
-retrying jupyter installation using anaconda
    -was able to get kernel working
    -have no idea how to get the notebook to start in preferred directory...

was finally able to fix target dir to start notebooks
    -open cwp.py file in anaconda directory
    -near end of file there is a line like this: os.chdir(documents_folder)
    -commented it out, changed to os.chdir('E:\\git-repos\\deep-learning-tutorial\\notebooks')
    -yay it works now



Image Classification
-computer sees images as matrix of pixels with RGB values
    -pixels_wide * pixels_tall * [R, G, B]
        -RGB are color channel values, integer from 0 (black) to 255 (white)
    -so basically, at pixel xy, the color is rbg
        -3d array of brightness values  --->    image label
-there are a lot of possible variations in how we perceive visual content
    -viewpoint - orientation with respect to camera
    -scale - size
    -deformation - things can be deformed/look different than usual
    -occlusion - parts may be hidden
    -illumination - lights affect pixel brightness
    -background clutter - object can blend into environment, harder to identify
    -intra-class variation - cats look different but still cats
-training dataset of labeled images
    -n = num images, k = num classes

-using pixel differences to compare images sucks because ultimately will group things based on similar colors
    -matrix is a list of pixel colors, so color is the biggest deciding factor

-nearest neighbor classifier
    -take test image, compare to all of training images, return label of closest training image
        -L1 distance - the sum of the differences between all vectors in two matrices; absolute value
        d1(I1,I2)=∑p|Ip1−Ip2|
            -pretty crappy classifier, better than at random but not human quality
        -L2 distance - euclidian distance between vectors, compute pixelwise differences, square them, then get sum, then get square root of sum
        d2(I1,I2)= sqrt(∑p((Ip1−Ip2)**2))
            -doesn't perform much better in image classification
            -L2 metric is more harsh than L1
    -has no training time, since just compare test objects to training objects
        -computationally expensive at test time
            -want most of load at training and efficient test instead

-k-nearest neighbor classifier
    -find top k closest images
        -return most frequent label? not sure if it's a pure frequency count
    -if k = 1, is basically just nearest neighbor
    -can use for hyperparameter tweaking
        -determine what value of k (num nearest neighbors to compare for prediction) gets best accuracy

-k-fold cross validation
    -if size of dataset is very small
    -divide dataset into k pieces, choose 1 for validation, use rest for training
        -iterate over all folds as validation, get eval, average performance across folds
        -only works if dataset is small because otherwise time consuming and computationally expensive
            if size of validation set small enough to quickly train/test on
                -for images probably 100-500 images
                -small text lines - about 1m lines

parametric approach is superior to kNN classifier because once you have parameters, don't need to keep train set
    -because not comparing test items to train items
    -parametric approach smaller because only requires matrix multiplication with W
        -W set during training, training data discard

Bias term
-bias term is what prevents f(xi) = 0 for resulting algorithm, weight matrix in shape k * 1 (k = num classes)
    -bias vector influences output scores for each class, but does not interact with data
        f(xi,W,b)=Wxi+b = linear mapping
        -basically prevents the classifier from being unable to make a decision on which class to pick, because each class has a bonus (can it have negative bonus?) probability of being the class
    -can set a default bias of 1 so don't have to keep track of bias weights
        -function becomes f(xi,W)=Wxi

Loss function
-loss function wants to minimize the differences between model's predictions and ground truth labels
    -adjusts hyperparameters until highest accuracy parameters found
-SVM loss
    -goal = correct class for image should have higher score than incorrect class by fixed margin delta
    Li=∑(j≠yi)max(0, sj − syi + Δ)
        -compute the loss for the i-th example using score for the j-th class
                -ultimately minimize loss by picking the class with the least shitty score
            -vector of class scores
            -syi = score of yi, true (gold) class score
            -sj = score of j-th class instance, predicted class score
            -Δ = its the fucking delta mate
        takes max between 0 or sum of difference between correct class score and predicted class score + delta
            -if correct class, should have significantly higher, non-zero score
                -how big are scores? integers or numbers smaller than one?
            -zero threshold is called hinge loss
                -standard version
                -there's also squared hinge loss max(0,−)**2, penalizes violated margins more strongly

    -Regularization penalty - if there are multiple possible values of W that correctly classify all examples, then they are all multiples of each other
        -uniform changes in W weights; penalty for using anything besides smallest common denominator

Delta
-kind of pointless
-can be set to 1 in pretty much all cases.
-why even freaking have it.
    -why.
    -Δ
    -apparently changing delta value just makes weights grow or shrink in weird ways, but is more regularization loss thing

So if the bias terms can all be 1, and the delta can be 1, then what actually matters for scores...


Softmax classifier
-has different loss function than SVM
    -svm uses hinge loss, softmax uses cross entropy loss
    -SVM scores are not easy to interpret (uncalibrated)
        -softmax scores can be used to compute probabilities of classes
            -because returns unnormalized log probabilities for each class
            -lambda value determines how regularized probs are
-softmax = normalization function
    -transform raw vectors into normalized positive values that sum to one so that cross entropy loss can be applied
-softmax is like logistic regression (which is inherently binary) but for multiple classes
    -I fucking love logistic regressions mate
-softmax classifier returns class scores that are the unnormalized log probabilities of the class
    -hinge loss replaced by cross entropy loss
        -cross entropy function
            H(p,q)=−∑xp(x)logq(x)
                -cross entropy between 'true' (gold) distribution p and estimated distrbution q
    -different from SVM where scores are more or less random
    Li = −log((efyi)/(∑jefj)) or equivalently Li = −fyi + log∑jefj
        -softmax function
        -transform vector of arbitrary real valued scores (z) to vector of values between zero and one, sum of vector values is one
-softmax classifier minimalizes cross entropy between estimated class probs and true class probs
    -mimimizing the negative log likelihood of the correct class
        -maximum likelihood estimation
        -regularization term R(w) = gaussian prior W
-softmax numeric stability
    -multiply function by constant C, push to sum
        ((efyi)/(∑jefj)) = ((Cefyi)/(C∑jefj)) = ((efyi + logC)/(∑jefj + logC))
    -can choose any value of C, will not change results but will improve numeric stability of function
    -common C value
        logC = −maxjfj
        -basically means shift values inside vector f so that highest value is zero

        f = np.array([123, 456, 789]) # example with 3 classes and each having large scores
        p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup

        # instead: first shift the values of f so that the highest number is 0:
        f -= np.max(f) # f becomes [-666, -333, 0]
        p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer



