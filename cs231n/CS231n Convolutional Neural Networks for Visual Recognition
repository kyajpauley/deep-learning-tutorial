http://cs231n.github.io/

This is probably just going to be a notes file, will make directories as needed for organization

Jupyter notebooks
-trying to run notebooks in a directory other than the default directory is really annoying
-was able to get it to run in repo notebooks directory with following command
    jupyter notebook --notebook-dir=..\..\..\git-repos\deep-learning-tutorial\notebooks
    -I don't know if that saved the default dir to there, will see next time start up
-to run jupyter notebooks from winpython, go to winpython folder, click shortcut
    -can also cd to file and run jupyter notebook from there

-having problems getting kernels to connect while using winpython version
    -location of notebooks dir doesn't seem to matter
-retrying jupyter installation using anaconda
    -was able to get kernel working
    -have no idea how to get the notebook to start in preferred directory...

was finally able to fix target dir to start notebooks
    -open cwp.py file in anaconda directory
    -near end of file there is a line like this: os.chdir(documents_folder)
    -commented it out, changed to os.chdir('E:\\git-repos\\deep-learning-tutorial\\notebooks')
    -yay it works now

-------------------------

Image Classification
-computer sees images as matrix of pixels with RGB values
    -pixels_wide * pixels_tall * [R, G, B]
        -RGB are color channel values, integer from 0 (black) to 255 (white)
    -so basically, at pixel xy, the color is rbg
        -3d array of brightness values  --->    image label
-there are a lot of possible variations in how we perceive visual content
    -viewpoint - orientation with respect to camera
    -scale - size
    -deformation - things can be deformed/look different than usual
    -occlusion - parts may be hidden
    -illumination - lights affect pixel brightness
    -background clutter - object can blend into environment, harder to identify
    -intra-class variation - cats look different but still cats
-training dataset of labeled images
    -n = num images, k = num classes

-using pixel differences to compare images sucks because ultimately will group things based on similar colors
    -matrix is a list of pixel colors, so color is the biggest deciding factor

-nearest neighbor classifier
    -take test image, compare to all of training images, return label of closest training image
        -L1 distance - the sum of the differences between all vectors in two matrices; absolute value
        d1(I1,I2)=∑p|Ip1−Ip2|
            -pretty crappy classifier, better than at random but not human quality
        -L2 distance - euclidian distance between vectors, compute pixelwise differences, square them, then get sum, then get square root of sum
        d2(I1,I2)= sqrt(∑p((Ip1−Ip2)**2))
            -doesn't perform much better in image classification
            -L2 metric is more harsh than L1
    -has no training time, since just compare test objects to training objects
        -computationally expensive at test time
            -want most of load at training and efficient test instead

-k-nearest neighbor classifier
    -find top k closest images
        -return most frequent label? not sure if it's a pure frequency count
    -if k = 1, is basically just nearest neighbor
    -can use for hyperparameter tweaking
        -determine what value of k (num nearest neighbors to compare for prediction) gets best accuracy

-k-fold cross validation
    -if size of dataset is very small
    -divide dataset into k pieces, choose 1 for validation, use rest for training
        -iterate over all folds as validation, get eval, average performance across folds
        -only works if dataset is small because otherwise time consuming and computationally expensive
            if size of validation set small enough to quickly train/test on
                -for images probably 100-500 images
                -small text lines - about 1m lines

-------------------------

parametric approach is superior to kNN classifier because once you have parameters, don't need to keep train set
    -because not comparing test items to train items
    -parametric approach smaller because only requires matrix multiplication with W
        -W set during training, training data discard

Bias term
-bias term is what prevents f(xi) = 0 for resulting algorithm, weight matrix in shape k * 1 (k = num classes)
    -bias vector influences output scores for each class, but does not interact with data
        f(xi,W,b)=Wxi+b = linear mapping
        -basically prevents the classifier from being unable to make a decision on which class to pick, because each class has a bonus (can it have negative bonus?) probability of being the class
    -can set a default bias of 1 so don't have to keep track of bias weights
        -function becomes f(xi,W)=Wxi

Loss function
-loss function wants to minimize the differences between model's predictions and ground truth labels
    -adjusts hyperparameters until highest accuracy parameters found
-SVM loss
    -goal = correct class for image should have higher score than incorrect class by fixed margin delta
    Li=∑(j≠yi)max(0, sj − syi + Δ)
        -compute the loss for the i-th example using score for the j-th class
                -ultimately minimize loss by picking the class with the least shitty score
            -vector of class scores
            -syi = score of yi, true (gold) class score
            -sj = score of j-th class instance, predicted class score
            -Δ = its the fucking delta mate
        takes max between 0 or sum of difference between correct class score and predicted class score + delta
            -if correct class, should have significantly higher, non-zero score
                -how big are scores? integers or numbers smaller than one?
            -zero threshold is called hinge loss
                -standard version
                -there's also squared hinge loss max(0,−)**2, penalizes violated margins more strongly

    -Regularization penalty - if there are multiple possible values of W that correctly classify all examples, then they are all multiples of each other
        -uniform changes in W weights; penalty for using anything besides smallest common denominator

Delta
-kind of pointless
-can be set to 1 in pretty much all cases.
-why even freaking have it.
    -why.
    -Δ
    -apparently changing delta value just makes weights grow or shrink in weird ways, but is more regularization loss thing

So if the bias terms can all be 1, and the delta can be 1, then what actually matters for scores...


Softmax classifier
-has different loss function than SVM
    -svm uses hinge loss, softmax uses cross entropy loss
    -SVM scores are not easy to interpret (uncalibrated)
        -softmax scores can be used to compute probabilities of classes
            -because returns unnormalized log probabilities for each class
            -lambda value determines how regularized probs are
-softmax = normalization function
    -transform raw vectors into normalized positive values that sum to one so that cross entropy loss can be applied
-softmax is like logistic regression (which is inherently binary) but for multiple classes
    -I fucking love logistic regressions mate
-softmax classifier returns class scores that are the unnormalized log probabilities of the class
    -hinge loss replaced by cross entropy loss
        -cross entropy function
            H(p,q)=−∑xp(x)logq(x)
                -cross entropy between 'true' (gold) distribution p and estimated distrbution q
    -different from SVM where scores are more or less random
    Li = −log((efyi)/(∑jefj)) or equivalently Li = −fyi + log∑jefj
        -softmax function
        -transform vector of arbitrary real valued scores (z) to vector of values between zero and one, sum of vector values is one
-softmax classifier minimalizes cross entropy between estimated class probs and true class probs
    -mimimizing the negative log likelihood of the correct class
        -maximum likelihood estimation
        -regularization term R(w) = gaussian prior W
-softmax numeric stability
    -multiply function by constant C, push to sum
        ((efyi)/(∑jefj)) = ((Cefyi)/(C∑jefj)) = ((efyi + logC)/(∑jefj + logC))
    -can choose any value of C, will not change results but will improve numeric stability of function
    -common C value
        logC = −maxjfj
        -basically means shift values inside vector f so that highest value is zero

        f = np.array([123, 456, 789]) # example with 3 classes and each having large scores
        p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup

        # instead: first shift the values of f so that the highest number is 0:
        f -= np.max(f) # f becomes [-666, -333, 0]
        p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer

-------------------------

How to optimize W?

-option 1 - pick random W values, test and see which ones work best
    -this is shitty
    -trying to randomly guess best possible set of W is dumb, better off taking an existing set of W and refining it during testing
-option 2 - random local search
    -start out with random W, change values randomly, test loss, if smaller make adjustment
    -this works okay, slightly better than option 1, but is still wasteful and computationally expensive
-option 3 - gradient
    -instead of picking random values to test a direction, compute best direction for weight changes
    -gradient loss function!
    -slope = instantaneous rate of change of the function at a given point, one dimensional function
        -gradient = slope for multidimensional vectors
            -gradient = vector of slopes (derivatives) for each dimension in input space
            -gradient is vector of partial derivatives in each dimension

think of loss function as high-dimensional optimization landscape
    -you are trying to get to the bottom as quickly as possible, but with a blindfold on
    -optimize loss function via iterative refinement
        -start with random set of weights, refine them step by step until loss is minimized
            -sounds like it could be computationally expensive
    -gradient of function gives steepest ascent direction
        -loss function wants to find gradients with least loss
    -parameter update has step size (learning rate, probably is alpha)
        -you can play with alpha in a lot of cases
            -controlled alpha decay
    -numerical vs analytic gradient
        -numerical gradient
            -simple, but imprecise and computationally expensive
        -analytic gradient
            -precise, fast to compute, more error prone because requires derivation of gradient math (more tricky to implement)
                -in practice always use analytic gradient then do gradient check
                    -implementation is compared to numerical gradient
    -gradient descent algorithm
        # Vanilla Gradient Descent
        while True:
            weights_grad = evaluate_gradient(loss_fun, data, weights)
            weights += - step_size * weights_grad # perform parameter update
        -most common way of optimizing neural network loss functions
    -mini batch gradient descent
        -large training sets have lots of examples, don't run loss over full set for single parameter update (computationally expensive)
            -instead use small batch (256 examples) to calculate loss
        # Vanilla Minibatch Gradient Descent
        while True:
            data_batch = sample_training_data(data, 256) # sample 256 examples
            weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
            weights += - step_size * weights_grad # perform parameter update
        -assumes that training samples are related
            -in practice not really a thing
            -could have multiple batches to consider
        -stocastic gradient descent (online gradient descent)
            -extreme version of mini batch containing only a single example that is evaluated
            -less common due to vectorized code optimizations making it more efficient to test on larger batch sizes (100+ examples)
                -people usually use term stochastic gradient descent to refer to this size more often than single sample size
                    -rarely see minibatch gradient descent term used, SGD is more colloquial

-------------------------

Backpropagation
    -compute gradients through recursive application of chain rule
    -important for neural networks

given f(x), x is input vector
    -compute gradient of f at x
        -relevant for loss function!
            -f = loss function
            -x = training data + neural network weights
        -ultimately backpropagation + loss function calculation is used to perform parameter updates
            -you can still use it on its own for visualization if you wanted
    -when computing gradient, computing partial derivatives
        -the rate of change of a function with respect to that variable
        -given f(x,y) = xy
            f = xy
            f/y = x --> ∂f/∂y = x
            f/x = y --> ∂f/∂x = y
            -is basically just algebra but there's an extra stupid derivative symbol thing making it look complicated
                -you can think of it as x is defined as the change in f over the change in y
                -or like any other math function... like pressure and volume and temperature, if you change any value the other values have to change too
            -calculating the derivative of a variable allows you to see how sensitive the function is with respect to that variable
                -multiplication
                    -f(x,y) = xy, x = 4 and y = -3
                        -f(x,y) = 12
                        -derivative of x = -3
                            -therefore, if increase value of x by tiny amount, the value of the entire expression decreases by 3 times that amount
                        -derivative of y = 4
                            -so, if increase y a tiny amount, entire function increased by 4 times that amount
                        -the derivative of y is x, the derivative of x is y
                -addition
                    -f(x,y) = x + y
                    -the derivative of both x and y is 1
                        -because addition
                        -if increase either variable, f increases by same amount
                -max operation
                    -f(x,y) = max(x,y)
                    -when x > y, ∂f/∂x = 1, else 0
                    -when y > x, ∂f/∂y = 1, else 0
                    -the subgradient is 1 for the larger input and 0 for the other(s)

Compound Expressions with Chain Rule
    -f(x,y,z)=(x+y)z ---> q=x+y and f=qz
-complicated expressions can be broken down into smaller expressions
    -multiplication, addition, max
    -multi:     f=xy ---> ∂f/∂x = y and ∂f/∂y = x
    -add:       f=x+y ---> ∂f/∂x = 1 and ∂f/∂y = 1
    -max:       f=max(x,y) ---> ∂f/∂x = 1(when x > y) and ∂f/∂y = 1(when y > x)
    -           f=1/x ---> ∂f/∂x = -1/x**2
-can compute derivatives for simpler functions seperately
    -  f=qz --->  ∂f/∂q=z and ∂f/∂z=q
    -  q=x+y --->  ∂q/∂x=1 and ∂q/∂y=1
-chain rule: recombine derivatives of smaller expressions through multiplication
    -∂f/∂x = (∂f/∂q)(∂q/∂x)
    -it sounds complicated but it really isn't
        -shit ends up being multiplied by 1 so it's okay

sigmoid function - σ(x)
-σ(x) this thing means sigmoid. if you see it as part of a function, it means sigmoid. dammit.
σ(x) = 1/(1 + e**−x) ---> (1 − σ(x)) * σ(x)
-full version:
    σ(x) = 1/(1+e**−x) ---> (dσ(x)/dx) = e**−x(1 + e**−x)**2 = ((1 + e***−x − 1)/(1 + e**−x)) * (1/(1 + e**−x)) = (1 − σ(x)) * σ(x)
-sigmoid function is popular because its derivation is really simple
-common choice for activation function of neural net
    -neurons fire, when they fire isn't important, but how often they fire is
        -rate code
        -model this firing rate with activation function f
        -sometimes firing timing is important for biological systems
-sigmoid function squishes larger numbers into a range between 0 and 1

staged backpropagation
-important to implement forward pass (main function) into simpler stages that are easy to backpropagate through
    -simpler functions have simpler derivations
    -you can use intermediate variables to compute

-------------------------

neurons
-lots of different types of neurons
-do function with input and weights, add bias and apply non-linearity (activation) function
    -sigmoid is common activation function
-single neuron classifier can behave like linear classifier with appropriate loss function
    -binary softmax classifier
        -probability of two classes must equal to 1
            -calculate cross entropy loss
            -prediction of classifier is based on whether the output of the neuron is greater than 0.5
                -due to sigmoid activation forcing values to be between 0 and 1
    -binary SVM classifier
        -max-marging loss function applied to output
    -regularization interpretation
        -regularization loss in SVM/softmax cases is analogous to gradual forgetting (biological)
        -drive all synaptic weights w toward zero after every parameter update
-commonly used activation functions
    -sigmoid
        -take real valued number, squish to range between 0 and 1
        -good interpretation of firing rate of neuron
            -either 0 or 1, not firing or fully saturated firing at max frequency (full binary, all or nothing)
        -recently unpopular
            -sigmoids saturate and kill gradients
                -if all values either 0 or 1, then all gradients in region are almost zero
                    -during backpropagation, local gradient is multiplied to entire function
                        -since basically 0, makes response also basically 0
                            -no signal flows from neuron to its weights or data
                            -so no updates to weights
                -if initial weights too large, most neurons in network will be oversaturated and not updated
            -sigmoid outputs not zero centered
                -sigmoid can only return positive values between 0 and 1
                -undesireable because later layers in the neural network would receive either all pos or all neg values (if gradient for whole expression is neg)
                -the final updates for the weights can have variable signs, and all batch gradients are added up at the end
                    -so not as bad as oversaturation
    -tanh
        - tanh(x) = 2σ(2x) − 1
        -take real valued number, squish to range between -1 and 1
            -activations saturate either 1 or -1, but the output is zero-centered since can be either pos or neg
                -therefore preferred over sigmoid
    -reLU
        -rectified linear unit
        - f(x) = max(0, x)
            -activation is thresholded at 0
        -pros
            -greatly accelerate convergence of stochastic gradient descent compared to sigmoid/tanh
                -due to being linear and non-saturating
            -isn't as computationally expensive as sigmoid/tanh because no exponents
            -easily implemented by thresholding a matrix of activations at zero (no neg values allowed)
        -cons
            -fragile during training
            -large gradient can cause weights to update in such a way that the neuron never activates again
                -from that point on, all gradient from that point is 0
            -reLU units can irreversibly die during training since can be knocked off data manifold
                -usually happens if learning rate is set too high
                    -not as much of an issue with a good alpha
        -should default to this neuron type, just be careful of learning rates and the possiblity of dead units
    -leaky reLU
        -tries to fix issue of everything negative going to 0, by instead making it go to -0.01
            -small constant a = slope
            f(x) = 1(x < 0)(αx) + 1(x > = 0)(x)
            -sort of kind of like a smoothing algorithm
    -maxout
        -does not have functional form where linearity is applied on dot product between weights and data
        -generalizes reLU and leaky version
        -max(wT1x + b1, wT2x + b2)
            -reLU and leaky reLU are special versions of this form
                -reLU, w1 and b1 = 0
            -no saturation, linear regime of operation
            -no dying units to worry about
    -summary
        -sigmoid sucks, don't use it
        -reLU is best, be careful of alpha and monitor for dying units
            -if having problems, try leaky reLU or maxout
            -can also try tanh, but it doesn't perform as well as reLU/maxout

neural network architectures + organization
-neurons in graphs
    -modeled as collections of neurons connected in acyclic graph
        -outputs of some neurons are inputs of other neurons
        -acyclic to avoid infinite loop
    -distinct layers of neurons
        -fully-connected layer
            -most common layer type
            -neurons between adjacent layers are fully pairwise connected, but neurons within single layer are unconnected
            to each other
                -neurons in the same layer do not have connections to each other, but have connections with neurons from other layers
-naming conventions
    -N-layer neural network
    -don't count the input layer in N
        -so single-layer = no hidden layers
            -input mapped directly to output
            -SVMs are special type of this
            -artificial neural networks (ANN), multilayer perceptrons (MLP)
    -neurons sometimes called units
-output layer
    -output layer neurons commonly do not have activation function
        -have linear identity activation function (technically)
    -output layer usually = class scores (classification), or real-valued target (regression)
-size
    -number of neurons, or number of parameters
    -convolutional neural networks tend to have 100+ parameters and 10-20 layers
        -number of effective connections is much greater due to parameter sharing

feed forward computation
-repeated matrix multiplication + interwoven activation function
    -if 3 layer network:
        - I  H  H
          I  H  H  O
          I  H  H
             H  H
        -3 matrices, one for each layer (1 input + 2 hidden)
            -hidden layer weights W + biases b
    -use matrix vector multiplication (dot product) to combine layers, add bias, then apply activation function

# forward-pass of a 3-layer neural network:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
out = np.dot(W3, h2) + b3 # output neuron (1x1)

representational power
-group of functions parameterized by the weights of the network
-universal approximator
    -for any continuous function and set of data, there exists some neural network with at least one hidden layer that approximates it
-usually 3 layer networks will outperform 2 layer networks
    -larger networks (4, 5, 6 layers) give diminishing returns in performance
        -opposite of convolutional networks, where more depth = better
            -mostly due to image classification, images are made up of composite parts so it makes sense to have more layers
            to represent those parts

number of layers + size of layers
-capacity of network increases as size increases
    -neural networks with more neurons can express more complicated functions
        -beware overfitting
            -fitting noise as well as data, doesn't allow for generalization
            -can decrease number of neurons to prevent overfitting
                -but there are better ways to control overfitting that should be tried first
                    -l2 regularization, dropout, input noise, etc.
                    -smaller networks are harder to train with gradient descent
                        -loss functions have relatively few local minima that are easier to converge to and lead to higher loss
                    -bigger networks have more local minima (but perform better in loss functions due to this)
                        -relies a lot less on luck of random initialization
                        -more paths to take through neural net, all relatively equally likely, so less chance of bad minima
-regularization strength is best way to control overfitting
    -lambda value
        -affects weight decay
    -smaller lambda (0.001 vs 0.1) is less regularized (more overfitting), while larger value is smoother edges

-------------------------

data and model (loss) set up
-neural networks perform a sequence of linear mappings interwoven with non-linear activation functions

preprocessing
-neural network takes matrix X as input
    -X is of size [N x D]
        -N = num samples
        -D = num dimensions
-common preprocessing techniques:
    -mean subtraction
        -subtract mean across every individual feature in data
            -centers cloud of data around the origin in every dimension (moves all values closer to 0)
        X -= np.mean(X, axis = 0)
            -can use X -= np.mean(X) to subtract a single value from all features
    -normalization
        -normalize data dimensions to be roughly the same scale
            -zero-center data, then divide each dimension by its standard deviation
                X /= np.std(X, axis = 0)
            -or
            -squish data to fit between -1 and 1
                -use this if input features have different scales but of are approximately equal importance to learning
                algorithm
                    -don't need to do this for images, since rbg color channels are all 0-255
    -pca and whitening
        -center data (subtract mean)
        -compute covariance matrix to find correlation structure in the data
            -diagonal of this matrix has variances
            -covariance matrix is symmetric and positive semi-definite
                -positive semi-definite = value is positive for every non-zero column vector, can have 0 values
        -compute SVD factorization of data covariance matrix
            -U = eigenvectors
                -columns are a set of orthonormal vectors
                    -norm of 1, orthogonal to each other
                        -a vector that when operated on by a given operator gives a scalar multiple of itself
                    -basis vector
            -S = 1-D array of singular values
        -decorrelate data by projecting original (zero-centered) data into eigenbasis
            -corresponds to rotation of the data in X so that new axes are the eigenvectors
                -covariance matrix of Xrot is now diagonal
            -np.lingalg.svd returns U values with eigenvector columns sorted by eigenvalues
                -can use to reduce dimensionality of data using only the top few eigenvectors
                    -scale down by dividing data by largest scales
                    -discard dimensions where the data has no variance ---> pca
                    -top eigenvectors capture the lower frequencies in the image
        -PCA = principal component analysis dimensionality reduction
            -reduce original dataset of size N x D to N x 100 (or however many eigenvectors you choose to keep)
            -re-express the image with a smaller dimensional matrix
                -loses fine detail, makes image more blurry but most information preserved
                    -removes nonessential elements
        -whitening
            -takes data on eigenbasis and divides every dimension by the eigenvalue to normalize the scale
                -all values are scaled by square root
                -add small constant to prevent division by zero (1e-5)
                    -can exaggerate noise in data because stretches all dimensions to be of equal size in input
                        -can be mitigated by stronger smoothing - increasing the constant to larger number
                -squishes variance along all eigenvalues to equal length
                    -lower frequencies (most variance) are now negligible
                    -high frequencies (least variance) are exaggerated
                        -it basically looks like a negative of the pca version, where all the fine details that were
                         removed are now visible

# Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
U,S,V = np.linalg.svd(cov)
Xrot = np.dot(X, U) # decorrelate the data
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100] pca
# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)


weight initialization
-do not set all initial weights to zero!
    -there is no asymmetry in neurons if their weights are initialized to be the same
-use small random numbers
    -symmetry breaking
        -neurons should be unique from each other so that any updates and correlations are deliberate rather than
        being due to previous neuron similarity
    -example: W = 0.01* np.random.randn(D,H)
        -standard deviation guassian distribution
            -recommended over using numbers drawn from uniform distribution, but has relatively little impact on final
            performance
    -don't use all smaller numbers
        -if weights are too small, can lead to very small gradients, which can diminish gradient signal and cause problems
        for larger networks
-normalize variance of each neuron
    -without normalization, variance grows by number of inputs (since they're all different and random)
    -scale weight vector by square root of num inputs
        w = np.random.randn(n) / sqrt(n)
            -if using ReLU units, use this instead
                w = np.random.randn(n) * sqrt(2.0/n)
    -ensures that all neurons wil have same output distribution
    -improves rate of convergence
-sparse initialization
    -could instead set all neuron weights to zero, but give them random connections to a fixed number of neurons below it
        -usually no smaller than 10
            -bigger networks
-bias initialization
    -can set to 0
    -can use small constant 0.01 for ReLU nonlinearities
        -ensures all ReLU units fire in the beginning
            -don't really *have* to, might be detrimental
-best practices:
    -ReLU units + w = np.random.randn(n) * sqrt(2.0/n) to initialize neuron weights
-batch normalization
    -common in neural networks
    -force activations to initialize with guassian distribution
        -sort of like doing preprocessing at every layer of network, but part of the network itself (rather than just the
        data)


regularization
-prevent overfitting!
-L2 regularization
    -penalize squared magnitude of all parameters
    -for every weight w in network, add 1/2λw**2
        -λ (lambda) is regularization strength
        -use 1/2 because it makes the gradient λw instead of 2λw
    -linear decay
        W += -lambda * W
    -final weight vectors are diffuse small numbers
    -usually superior performance over L1
-L1 regularization
    -for every weight w in network, add λ∣w∣
    -elastic net regularization
        -combine L2 and L1
        λ1∣w∣+λ2w**2
            -I think there's two different values λ1 and λ2
    -weight vectors become sparse during optimization
        -neurons use only sparse subset of most important inputs
            -reduce noise
-max norm constraints
    -enforce upper limit on magnitude of weight vector for every neuron
        -use projected gradient descent to enforce constraint
    -perform parameter update as normal, then enforce constraint
        -make sure output is less than c
            -c is usually 3 or 4
    -prevents network from "exploding" if learning rates are set too high
-dropout
    -hyperparameter p = probability
    -neuron is only active within that probability, otherwise zero
    -sample a neural network within a larger neural network (pick some neurons)
        -only update the sample neurons based on input data
            -don't do dropout during testing
        -sub-networks?
    -perform dropout for each hidden layer
        -can also do for input layer, need binary mask for input X
    -during prediction, scale hidden layer output by p
        -necessary in order to have the same output during training
    -better to use inverted dropout
        -perform p scaling during training so not needed during testing
    -introduces stochastic gradient behaviour in forward pass of network
        -noise marginalized over analytically
            -multiplying by p
        -or numerically
            -sampling
                -pick samples, perform several forward passes and average results
    -DropConnect
        -random set of weights set to zero during forward pass
            -instead of turning of neurons, turn off random weights instead
-bias
    -rare to regularize bias because doesn't interact with data multiplicatively
    -still can manipulate if desired to reduce loss
-can also regularize individual layers
    -no one really does this
-best practices
    -L2 regularization, do cross validation to find best lambda value
    -also do dropout for all layers
        p = 0.5
            -good default

loss functions
-data loss
    -measures compatibility between prediction and ground truth
        -average over data losses for every sample
    L = 1/N∑iLi
        N = number of training data
-classification
    -SVM
        Li = ∑(j≠yi)max(0, fj − fyi + 1)
        -squared hinge loss can have better performance
            max(0, fj − fyi +1 )**2
    -softmax classifier + cross-entropy loss
        Li = −log(efyi / ∑jefj)
        -hierarchical softmax
            -better for large number of classes
            -tree of labels, each label represented as path along tree
                -softmax classifier trained at every node of tree to disambiguate left and right branch
-attribute classification
    -single item can have multiple labels, nonexclusive
    -build binary classifier for each attribute
        Li = ∑jmax(0, 1 − yijfj)
            -the sum of all categories j and yij is +1 or -1 depending on if the i-th example has the j-th attribute
            -score vector fj is positive if predicted to be present and negative otherwise
            -loss accumulated if positive example has score < 1 or negative example has score > -1
    -build logistic regression classifier for each attribute
        P(y = 1∣x;w,b) = 1/(1+e**−(wTx+b)) = σ(wTx+b)
        P(y = 0∣x;w,b) = 1 − P(y=1∣x;w,b)
        -loss function maximizes log likelihood of probability of positive answer
            Li = ∑(j)yij log(σ(fj)) + (1 − yij)log(1 − σ(fj))
                -sigmoid function!
            -gradient:
                ∂Li/∂fj = yij − σ(fj)
-regression
    -predicting real valued quantities
    -compute the loss between predicted quantity and true answer
        -measure L2 squared norm or L1 norm of difference
        -L2 squared norm:
            Li = ∥f−yi∥22   # it's not a 22, they're stacked on each other
            -gradient:
                δij or sign(δij)
                -gradient will either be directly proportional to difference in the error, or will be fixed and only
                inherit the sign of the difference
            -harder to optimize than softmax
                -tbh might be easier to build binary classifiers if number of possible class values small/discrete enough
        -L1 norm:
            Li = ∥f−yi∥1 = ∑j∣fj−(yi)j∣
                -∑j is sum over all dimensions of desired prediction, if there is more than one quantity predicted
-structured prediction
    -labels can be arbitrary structures (graphs, trees, other visualizations)
    -structured SVM loss
        -margin between correct structure yi and highest-scoring incorrect structure

summary
-preprocess by centering data (mean of zero), normalize scale to [-1, 1] for each dimension (feature)
-initialize weights with guassian distribution
    -standard deviation of √(2/n)
        -n = num inputs to neuron
        w = np.random.randn(n) * sqrt(2.0/n)
-L2 regularization + dropout
-batch normalization

-------------------------

learning and evaluation

gradient checks
-compare analytic (calculated) gradient to numerical (actual) gradient
-centered formula
    df(x)/dx = (f(x + h) − f(x − h))/2h
        -use 2h instead of 1 h because h tends to be really small number (too close to zero)
        -have to evaluate loss function twice (more computationally expensive) but more precise
-relative error
    -need to scale error based on difference in scales between numerical and analytic gradient
    ∣f′a−f′n∣ / max(∣f′a∣,∣f′n∣)
    -compare ratio of differences to ratio of absolute values of both gradients
    -error rates in practice:
        -relative error > 1e-2 usually means the gradient is probably wrong
        - 1e-2 > relative error > 1e-4 should make you feel uncomfortable
        - 1e-4 > relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh
            nonlinearities and softmax), then 1e-4 is too high.
        - 1e-7 and less you should be happy
     -deeper the network, higher the relative errors
-double precision
-stay around active range of floating point
    -beware tiny numbers!
-kinks
    -kinks = non-differentiable parts of objective function
        -ReLU, SVM loss, Maxout neurons, etc.
        -instead of getting analytic gradient of 0, numerical gradient can be non-zero if h value high enough
    -keep track of "winners" in max functions for forward pass, compare results for backpropagation
        -if winner changes, then found kink, numerical gradient will not be exact
    -can fix kinks by having fewer datapoints
        -having fewer datapoints can make gradient check faster and more efficient
-h
    -step size!
    -usually around 1e**-4 to 1e**-6
-use realistic weight initializations for gradient checks
    -beware pathological situations caused by unlucky intializations/combinations
    -use short burn-in time, perform gradient check after loss goes down
        -don't gradient check on first iteration
            -can have weird edge cases
-don't let regularization overwhelm data
    -if loss function is data loss + regularization loss, regularization loss can overwhelm data loss
        -gradients primarily coming from regularization term
    -recommended to turn off regularization and check data loss alone first, then independently check regularization
        -remove dataloss contribution to get regularization contribution
        -could also increase regularization strength to create non-negligible effect in gradient check
-turn off dropout/augmentations during gradient check
    -if you don't turn them off could have huge errors in calculating numeric gradient
    -however makes so you can't gradient check properly (need to make sure dropout is backpropagated properly)
        -try forcing particular random seed before evaluating both f(x + h) and f(x - h), and when evaluating analytic
        gradient
-sample gradients to check
    -don't need to check all of them
    -don't sample at random, sample deliberately

sanity checks
-check data loss at/before initialization
    -set regularization strength to zero
    -if not getting expected loss then points to problem with initialization
-increasing regularization strength = increase loss
-overfit on tiny subset of data
    -attempt to obtain zero loss on small subset (20 samples)
        -set regularization to zero
        -if cannot achieve 0 loss on small dataset, not worth persuing with larger set

monitoring during learning process
-loss function
    -measured as loss per epoch
        -loss on y, epoch on x; should go down
    -low learning rate = linear improvement
    -high learning rate = faster loss decay, but improvements stop sooner
        -too much energy in optimization, parameters unable to settle
        -very high learning rate = loss gets stupid high
-train/val accuracy
    -measured as accuracy per epoch
        -accuracy on y, epoch on x; should go up
    -gap between accuracy on training versus validation set indicates degree of overfitting
    -large gap between val and train = strong overfitting
        -increase regularization (stronger L2, increase dropout, etc)
        -add more data
    -small gap between val and train = little overfitting
        -model capacity not high enough
        -make model larger by increasing number of parameters
-ratio of weights to updates
    -track ratio of update magnitudes to value magnitudes
        -use updates, not raw gradients
            -vanilla stochastic gradient descent = gradient * learning rate
    -evaluate and track for each set of parameters
    -aim for 1e-3
        # assume parameter vector W and its gradient vector dW
        param_scale = np.linalg.norm(W.ravel())
        update = -learning_rate*dW # simple SGD update
        update_scale = np.linalg.norm(update.ravel())
        W += update # the actual update
        print update_scale / param_scale # want ~1e-3
    -can track min or max, or compute and track norm of gradients + updates
        -usually correlated, should give similar results
-activation / gradient distribution per layer
    -plot activation/gradient histograms for all layers of network
        -if have strange distribution of neuron activations then it's messed up
-first layer visualizations
    -plot first layer features visually
    -noisy features can indicate unconverged network, improperly set learning rate, or very low weight regularization
    penalty
    -smooth features = good

parameter updates
-compute analytic gradient with backpropagation, then use gradients to perform parameter update
-stochastic gradient descent (sgd)
    -vanilla update
        -change parameters along negative gradient direction
            # Vanilla update
            x += - learning_rate * dx
        -learning rate = hyperparameter (fixed constant)
            -if low enough, guaranteed to make non-negative process on loss function
    -momentum update
        -better convergence rates on deep networks
        -initialize parameters with random numbers
            -set a particle with zero initial velocity at some random location in gradient landscape
                -optimization pushes particle along on landscape (like hills)
                -force felt by particle is negative gradient of loss function
                    -negative gradient is proportional to acceleration of particle
                    -gradient directly influences velocity, which affects position of particle
                        # Momentum update
                        v = mu * v - learning_rate * dx # integrate velocity
                        x += v # integrate position
                    -v is variable initialized at zero
                    -mu dampens velocity and reduces kinetic energy of system
                        -hyperparameter mu is momentum (coefficient of friction)
                        -makes particle stop moving at bottom of hill
                        -usually cross validated at values [0.5, 0.9, 0.95, 0.99]
                        -optimization can sometimes benefit from momentum schedules
                            -increase momentum in later stages of learning (increase as epochs increase)
        -parameter vector will build up velocity in any direction with consistent gradient
    -Nesterov momentum
        -similar to momentum update, stronger convergence for convex functions
            -works slightly better than normal momentum
        -if current parameter vector is at position x, we know momentum will affect vector by mu * v
            -future approximations = x + mu * v = lookahead
                -compute analytic gradient at lookahead point rather than x
                    x_ahead = x + mu * v
                    # evaluate dx_ahead (the gradient at x_ahead instead of at x)
                    v = mu * v - learning_rate * dx_ahead
                    x += v
                -in practice want it to be more similar to vanilla sgd update
                    v_prev = v # back this up
                    v = mu * v - learning_rate * dx # velocity update stays the same
                    x += -mu * v_prev + (1 + mu) * v # position update changes form
-annealing the learning rate
    -anneal = update learning rate over time
    -step decay
        -reduce learning rate by factor every few epochs
            -example: halve the learning rate every 5 epochs
            -example: reduce learning rate by 0.1 every 20 epochs
            etc.
        -in practice, watch validation error rate during training, reduce learning rate by constant (0.5) when validation
        error rate stops improving
    -exponential decay
        α = α0e**−kt
            -a0 and k are hyperparameters
            -t = iteration number (or number of epochs)
    -1/t decay
        α = α0 / (1 + kt)
    -step decay is usually prefered method
        -ultimately better to have slower decay over longer training time
-second order methods
    -newton's method
        x ← x − [Hf(x)]**−1 ∇f(x)
            Hf(x) = Hessian matrix
                -square matrix of second-order partial derivatives of function
                -local curvature of loss function
                    -more efficient update
                    -multiplying by inverse Hessian makes optimization take larger steps in directions of
                    shallow curvature and shorter steps in directions of steep curvature
            ∇f(x) = gradient vector
        -no hyperparameters to adjust
        -impractical for deep learning applications because computing Hessian takes long fucking time and it's too big
            -instead use quasi-Newton methods
                -L-BFGS
                    -uses information in gradients over time to compute approximation of Hessian matrix
                        -doesn't compute full matrix so it's less computationally expensive/big af
                    -needs to be computed over entire training set, which could still be huge and suck
                        -not very many methods to use on smaller batches
    -in practice, use SGD momentum methods because simpler and more easily scalable
-per-parameter adaptive update methods
    -instead of updating all parameters with the same learning rate, you can update them individually
    -adagrad
            # Assume the gradient dx and parameter vector x
            cache += dx**2
            x += - learning_rate * dx / (np.sqrt(cache) + eps)
        -cache is the same size as the gradient
            -per-parameter sum of squared updates
            -normalize parameter update step element-wise
            -weights with high gradients = learning rate reduced
                -weights with small gradients/infrequent updates = effective learning rate increased
        -eps = smoothing term
            -usually between 1e-4 and 1e-8
            -avoid division by zero
        -can potentially update too aggressively early on and stop learning more quickly
    -RMSprop
        -supposed to be less aggressive and monotonic than adagrad
        -uses moving average of squared gradients
            cache = decay_rate * cache + (1 - decay_rate) * dx**2
            x += - learning_rate * dx / (np.sqrt(cache) + eps)
        -decay rate = hyperparameter
            -[0.9, 0.99, 0.999] = typical values
        -cache variable is leaky
    -adam
        -RMSprop + momentum
            m = beta1*m + (1-beta1)*dx
            v = beta2*v + (1-beta2)*(dx**2)
            x += - learning_rate * m / (np.sqrt(v) + eps)
        -m = smoothed version of gradient (dx is raw, noisy gradient)
        -recommended values:
            eps = 1e-8
            beta1 = 0.9
            beta2 = 0.999
        -recommended algorithm to use!
        -bias correction algorithm
            -compensates for m and v being initialized at zero, accounts for burn in time
            # t is your iteration counter going from 1 to infinity
            m = beta1*m + (1-beta1)*dx
            mt = m / (1-beta1**t)
            v = beta2*v + (1-beta2)*(dx**2)
            vt = v / (1-beta2**t)
            x += - learning_rate * mt / (np.sqrt(vt) + eps)

hyperparameter optimization
-initial learning rate, learning decay schedule (decay constant), regularization strength (L2, dropout)
-implementation
    -neural networks require a long time to train (fml)
    -worker and master
        -worker - continuously samples random hyperparameters and performs optimization
            -get validation performance per epoch
                -write to file as model checkpoint
                    -log!
                    -put val performance in filename
        -master - launches or kills workers on computing cluster
            -inspect model checkpoints
            -plot training statistics
-single validation set > cross validation
    -better to have single, large validation set if possible
    -can do cross validation with multiple folds, but due to training/testing time it's slow
-hyperparameter ranges
    -log scale
        -ex: learning_rate = 10 ** uniform(-6, 1)
            -take random number from uniform distribution and use as exponent for 10
        -good for learning rate and regularization strength
            -these parameters have multiplicative effects on data
                -consider range of learning rate multiplied or divided by some value rather than adding/subtracting a constant
    -dropout = use normal scale
        ex: dropout = uniform(0,1)
-random search > grid search
    -random samples better than scheduled samples
        -easier to implement
-beward border values
    -when sampling from a distribution (ex: uniform(-6, 1)), check to make sure final learning rate is not at interval edges
    -might be more optimal hyperparameter beyond interval
-coarse search ---> fine search
    -start with sampling in coarse range (ex: [-6, 1]) then use narrower range for tuning
        -coarse search best for first few epochs
        -narrower range for 5 epochs
        -final narrow range for rest of epochs
-bayesian hyperparameter optimization
    -libraries to use:
        -spearmint
        -SMAC
        -Hyperopt
    -in practice, still better to use random hyperparameter search

evaluation
model ensembles
-train multiple independent models, average predictions at test time
    -as num models increases, performance increases monotonically (diminishing returns)
    -higher variety of models in ensemble = more dramatic improvements
-same model, different initializations
    -cross validation to get best hyperparameters
    -train multiple models with best hyperparameters but different random initialization
-top models from cross validation
    -cross validation to get best hyperparameters
        -pick best models (top 10) to create ensemble
    -improves variety, risk of suboptimal models
        -doesn't need additional cross validation or retraining
-single model, different checkpoints
    -best for expensive training
    -take different checkpoints of network over time (epochs)
    -not very variable, but works well
-running average of training parameters
    -maintain copy of network's weights in memory
        -maintain exponentially decaying sum of previous weights
    -average state of network over last several iterations
    -smoothed
        -usually has better validation error than unsmoothed
-model ensembles can be slow during evaluation

best practices
-gradient check implementation with small batch of data
-sanity check
    -make sure good initial loss
    -achieve 100% accuracy on small subset of data
-monitor loss, training/validation accuracy, and magnitude of updates to parameter values
    -1e-3
    -conv nets should use first layer weights
-update with sgd + nesterov momentum or adam
-decay learning rate over epochs
    -halve learning rate after fixed num epochs or when validation accuracy plateaus
-sample hyperparameters with random search
    -coarse (1-5 epochs) ---> fine
-ensemble models can have extra performance