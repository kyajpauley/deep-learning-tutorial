http://cs231n.github.io/

This is probably just going to be a notes file, will make directories as needed for organization

Jupyter notebooks
-trying to run notebooks in a directory other than the default directory is really annoying
-was able to get it to run in repo notebooks directory with following command
    jupyter notebook --notebook-dir=..\..\..\git-repos\deep-learning-tutorial\notebooks
    -I don't know if that saved the default dir to there, will see next time start up
-to run jupyter notebooks from winpython, go to winpython folder, click shortcut
    -can also cd to file and run jupyter notebook from there

-having problems getting kernels to connect while using winpython version
    -location of notebooks dir doesn't seem to matter
-retrying jupyter installation using anaconda
    -was able to get kernel working
    -have no idea how to get the notebook to start in preferred directory...

was finally able to fix target dir to start notebooks
    -open cwp.py file in anaconda directory
    -near end of file there is a line like this: os.chdir(documents_folder)
    -commented it out, changed to os.chdir('E:\\git-repos\\deep-learning-tutorial\\notebooks')
    -yay it works now

-------------------------

Image Classification
-computer sees images as matrix of pixels with RGB values
    -pixels_wide * pixels_tall * [R, G, B]
        -RGB are color channel values, integer from 0 (black) to 255 (white)
    -so basically, at pixel xy, the color is rbg
        -3d array of brightness values  --->    image label
-there are a lot of possible variations in how we perceive visual content
    -viewpoint - orientation with respect to camera
    -scale - size
    -deformation - things can be deformed/look different than usual
    -occlusion - parts may be hidden
    -illumination - lights affect pixel brightness
    -background clutter - object can blend into environment, harder to identify
    -intra-class variation - cats look different but still cats
-training dataset of labeled images
    -n = num images, k = num classes

-using pixel differences to compare images sucks because ultimately will group things based on similar colors
    -matrix is a list of pixel colors, so color is the biggest deciding factor

-nearest neighbor classifier
    -take test image, compare to all of training images, return label of closest training image
        -L1 distance - the sum of the differences between all vectors in two matrices; absolute value
        d1(I1,I2)=∑p|Ip1−Ip2|
            -pretty crappy classifier, better than at random but not human quality
        -L2 distance - euclidian distance between vectors, compute pixelwise differences, square them, then get sum, then get square root of sum
        d2(I1,I2)= sqrt(∑p((Ip1−Ip2)**2))
            -doesn't perform much better in image classification
            -L2 metric is more harsh than L1
    -has no training time, since just compare test objects to training objects
        -computationally expensive at test time
            -want most of load at training and efficient test instead

-k-nearest neighbor classifier
    -find top k closest images
        -return most frequent label? not sure if it's a pure frequency count
    -if k = 1, is basically just nearest neighbor
    -can use for hyperparameter tweaking
        -determine what value of k (num nearest neighbors to compare for prediction) gets best accuracy

-k-fold cross validation
    -if size of dataset is very small
    -divide dataset into k pieces, choose 1 for validation, use rest for training
        -iterate over all folds as validation, get eval, average performance across folds
        -only works if dataset is small because otherwise time consuming and computationally expensive
            if size of validation set small enough to quickly train/test on
                -for images probably 100-500 images
                -small text lines - about 1m lines

-------------------------

parametric approach is superior to kNN classifier because once you have parameters, don't need to keep train set
    -because not comparing test items to train items
    -parametric approach smaller because only requires matrix multiplication with W
        -W set during training, training data discard

Bias term
-bias term is what prevents f(xi) = 0 for resulting algorithm, weight matrix in shape k * 1 (k = num classes)
    -bias vector influences output scores for each class, but does not interact with data
        f(xi,W,b)=Wxi+b = linear mapping
        -basically prevents the classifier from being unable to make a decision on which class to pick, because each class has a bonus (can it have negative bonus?) probability of being the class
    -can set a default bias of 1 so don't have to keep track of bias weights
        -function becomes f(xi,W)=Wxi

Loss function
-loss function wants to minimize the differences between model's predictions and ground truth labels
    -adjusts hyperparameters until highest accuracy parameters found
-SVM loss
    -goal = correct class for image should have higher score than incorrect class by fixed margin delta
    Li=∑(j≠yi)max(0, sj − syi + Δ)
        -compute the loss for the i-th example using score for the j-th class
                -ultimately minimize loss by picking the class with the least shitty score
            -vector of class scores
            -syi = score of yi, true (gold) class score
            -sj = score of j-th class instance, predicted class score
            -Δ = its the fucking delta mate
        takes max between 0 or sum of difference between correct class score and predicted class score + delta
            -if correct class, should have significantly higher, non-zero score
                -how big are scores? integers or numbers smaller than one?
            -zero threshold is called hinge loss
                -standard version
                -there's also squared hinge loss max(0,−)**2, penalizes violated margins more strongly

    -Regularization penalty - if there are multiple possible values of W that correctly classify all examples, then they are all multiples of each other
        -uniform changes in W weights; penalty for using anything besides smallest common denominator

Delta
-kind of pointless
-can be set to 1 in pretty much all cases.
-why even freaking have it.
    -why.
    -Δ
    -apparently changing delta value just makes weights grow or shrink in weird ways, but is more regularization loss thing

So if the bias terms can all be 1, and the delta can be 1, then what actually matters for scores...


Softmax classifier
-has different loss function than SVM
    -svm uses hinge loss, softmax uses cross entropy loss
    -SVM scores are not easy to interpret (uncalibrated)
        -softmax scores can be used to compute probabilities of classes
            -because returns unnormalized log probabilities for each class
            -lambda value determines how regularized probs are
-softmax = normalization function
    -transform raw vectors into normalized positive values that sum to one so that cross entropy loss can be applied
-softmax is like logistic regression (which is inherently binary) but for multiple classes
    -I fucking love logistic regressions mate
-softmax classifier returns class scores that are the unnormalized log probabilities of the class
    -hinge loss replaced by cross entropy loss
        -cross entropy function
            H(p,q)=−∑xp(x)logq(x)
                -cross entropy between 'true' (gold) distribution p and estimated distrbution q
    -different from SVM where scores are more or less random
    Li = −log((efyi)/(∑jefj)) or equivalently Li = −fyi + log∑jefj
        -softmax function
        -transform vector of arbitrary real valued scores (z) to vector of values between zero and one, sum of vector values is one
-softmax classifier minimalizes cross entropy between estimated class probs and true class probs
    -mimimizing the negative log likelihood of the correct class
        -maximum likelihood estimation
        -regularization term R(w) = gaussian prior W
-softmax numeric stability
    -multiply function by constant C, push to sum
        ((efyi)/(∑jefj)) = ((Cefyi)/(C∑jefj)) = ((efyi + logC)/(∑jefj + logC))
    -can choose any value of C, will not change results but will improve numeric stability of function
    -common C value
        logC = −maxjfj
        -basically means shift values inside vector f so that highest value is zero

        f = np.array([123, 456, 789]) # example with 3 classes and each having large scores
        p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup

        # instead: first shift the values of f so that the highest number is 0:
        f -= np.max(f) # f becomes [-666, -333, 0]
        p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer

-------------------------

How to optimize W?

-option 1 - pick random W values, test and see which ones work best
    -this is shitty
    -trying to randomly guess best possible set of W is dumb, better off taking an existing set of W and refining it during testing
-option 2 - random local search
    -start out with random W, change values randomly, test loss, if smaller make adjustment
    -this works okay, slightly better than option 1, but is still wasteful and computationally expensive
-option 3 - gradient
    -instead of picking random values to test a direction, compute best direction for weight changes
    -gradient loss function!
    -slope = instantaneous rate of change of the function at a given point, one dimensional function
        -gradient = slope for multidimensional vectors
            -gradient = vector of slopes (derivatives) for each dimension in input space
            -gradient is vector of partial derivatives in each dimension

think of loss function as high-dimensional optimization landscape
    -you are trying to get to the bottom as quickly as possible, but with a blindfold on
    -optimize loss function via iterative refinement
        -start with random set of weights, refine them step by step until loss is minimized
            -sounds like it could be computationally expensive
    -gradient of function gives steepest ascent direction
        -loss function wants to find gradients with least loss
    -parameter update has step size (learning rate, probably is alpha)
        -you can play with alpha in a lot of cases
            -controlled alpha decay
    -numerical vs analytic gradient
        -numerical gradient
            -simple, but imprecise and computationally expensive
        -analytic gradient
            -precise, fast to compute, more error prone because requires derivation of gradient math (more tricky to implement)
                -in practice always use analytic gradient then do gradient check
                    -implementation is compared to numerical gradient
    -gradient descent algorithm
        # Vanilla Gradient Descent
        while True:
            weights_grad = evaluate_gradient(loss_fun, data, weights)
            weights += - step_size * weights_grad # perform parameter update
        -most common way of optimizing neural network loss functions
    -mini batch gradient descent
        -large training sets have lots of examples, don't run loss over full set for single parameter update (computationally expensive)
            -instead use small batch (256 examples) to calculate loss
        # Vanilla Minibatch Gradient Descent
        while True:
            data_batch = sample_training_data(data, 256) # sample 256 examples
            weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
            weights += - step_size * weights_grad # perform parameter update
        -assumes that training samples are related
            -in practice not really a thing
            -could have multiple batches to consider
        -stocastic gradient descent (online gradient descent)
            -extreme version of mini batch containing only a single example that is evaluated
            -less common due to vectorized code optimizations making it more efficient to test on larger batch sizes (100+ examples)
                -people usually use term stochastic gradient descent to refer to this size more often than single sample size
                    -rarely see minibatch gradient descent term used, SGD is more colloquial

-------------------------

Backpropagation
    -compute gradients through recursive application of chain rule
    -important for neural networks

given f(x), x is input vector
    -compute gradient of f at x
        -relevant for loss function!
            -f = loss function
            -x = training data + neural network weights
        -ultimately backpropagation + loss function calculation is used to perform parameter updates
            -you can still use it on its own for visualization if you wanted
    -when computing gradient, computing partial derivatives
        -the rate of change of a function with respect to that variable
        -given f(x,y) = xy
            f = xy
            f/y = x --> ∂f/∂y = x
            f/x = y --> ∂f/∂x = y
            -is basically just algebra but there's an extra stupid derivative symbol thing making it look complicated
                -you can think of it as x is defined as the change in f over the change in y
                -or like any other math function... like pressure and volume and temperature, if you change any value the other values have to change too
            -calculating the derivative of a variable allows you to see how sensitive the function is with respect to that variable
                -multiplication
                    -f(x,y) = xy, x = 4 and y = -3
                        -f(x,y) = 12
                        -derivative of x = -3
                            -therefore, if increase value of x by tiny amount, the value of the entire expression decreases by 3 times that amount
                        -derivative of y = 4
                            -so, if increase y a tiny amount, entire function increased by 4 times that amount
                        -the derivative of y is x, the derivative of x is y
                -addition
                    -f(x,y) = x + y
                    -the derivative of both x and y is 1
                        -because addition
                        -if increase either variable, f increases by same amount
                -max operation
                    -f(x,y) = max(x,y)
                    -when x > y, ∂f/∂x = 1, else 0
                    -when y > x, ∂f/∂y = 1, else 0
                    -the subgradient is 1 for the larger input and 0 for the other(s)

Compound Expressions with Chain Rule
-complicated expressions can be broken down into smaller expressions (multiplication, addition or max)
    -f(x,y,z)=(x+y)z ---> q=x+y and f=qz
-can compute derivatives for simpler functions seperately
    -  f=qz --->  ∂f/∂q=z and ∂f/∂z=q
    -  q=x+y --->  ∂q/∂x=1 and ∂q/∂y=1
-chain rule: recombine derivatives of smaller expressions through multiplication
    -∂f/∂x = (∂f/∂q)(∂q/∂x)
    -it sounds complicated but it really isn't
        -shit ends up being multiplied by 1 so it's okay

sigmoid function - σ(x)
-σ(x) this thing means sigmoid. if you see it as part of a function, it means sigmoid. dammit.
σ(x) = 1/(1 + e**−x) ---> (1 − σ(x)) * σ(x)
-full version:
    σ(x) = 1/(1+e**−x) ---> (dσ(x)/dx) = e**−x(1 + e**−x)**2 = ((1 + e***−x − 1)/(1 + e**−x)) * (1/(1 + e**−x)) = (1 − σ(x)) * σ(x)
-sigmoid function is popular because its derivation is really simple
-common choice for activation function of neural net
    -neurons fire, when they fire isn't important, but how often they fire is
        -rate code
        -model this firing rate with activation function f
        -sometimes firing timing is important for biological systems
-sigmoid function squishes larger numbers into a range between 0 and 1

staged backpropagation
-important to implement forward pass (main function) into simpler stages that are easy to backpropagate through
    -simpler functions have simpler derivations
    -you can use intermediate variables to compute

-------------------------

neurons
-lots of different types of neurons
-do function with input and weights, add bias and apply non-linearity (activation) function
    -sigmoid is common activation function
-single neuron classifier can behave like linear classifier with appropriate loss function
    -binary softmax classifier
        -probability of two classes must equal to 1
            -calculate cross entropy loss
            -prediction of classifier is based on whether the output of the neuron is greater than 0.5
                -due to sigmoid activation forcing values to be between 0 and 1
    -binary SVM classifier
        -max-marging loss function applied to output
    -regularization interpretation
        -regularization loss in SVM/softmax cases is analogous to gradual forgetting (biological)
        -drive all synaptic weights w toward zero after every parameter update
-commonly used activation functions
    -sigmoid
        -take real valued number, squish to range between 0 and 1
        -good interpretation of firing rate of neuron
            -either 0 or 1, not firing or fully saturated firing at max frequency (full binary, all or nothing)
        -recently unpopular
            -sigmoids saturate and kill gradients
                -if all values either 0 or 1, then all gradients in region are almost zero
                    -during backpropagation, local gradient is multiplied to entire function
                        -since basically 0, makes response also basically 0
                            -no signal flows from neuron to its weights or data
                            -so no updates to weights
                -if initial weights too large, most neurons in network will be oversaturated and not updated
            -sigmoid outputs not zero centered
                -sigmoid can only return positive values between 0 and 1
                -undesireable because later layers in the neural network would receive either all pos or all neg values (if gradient for whole expression is neg)
                -the final updates for the weights can have variable signs, and all batch gradients are added up at the end
                    -so not as bad as oversaturation
    -tanh
        - tanh(x) = 2σ(2x) − 1
        -take real valued number, squish to range between -1 and 1
            -activations saturate either 1 or -1, but the output is zero-centered since can be either pos or neg
                -therefore preferred over sigmoid
    -reLU
        -rectified linear unit
        - f(x) = max(0, x)
            -activation is thresholded at 0
        -pros
            -greatly accelerate convergence of stochastic gradient descent compared to sigmoid/tanh
                -due to being linear and non-saturating
            -isn't as computationally expensive as sigmoid/tanh because no exponents
            -easily implemented by thresholding a matrix of activations at zero (no neg values allowed)
        -cons
            -fragile during training
            -large gradient can cause weights to update in such a way that the neuron never activates again
                -from that point on, all gradient from that point is 0
            -reLU units can irreversibly die during training since can be knocked off data manifold
                -usually happens if learning rate is set too high
                    -not as much of an issue with a good alpha
    -leaky reLU
        -tries to fix issue of everything negative going to 0, by instead making it go to 0.1